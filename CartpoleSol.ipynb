{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python38364bitd28d5bfd303b43519c8e7d8f134c3bbd",
   "display_name": "Python 3.8.3 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import gym\n",
    "import random\n",
    "import itertools\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop\n",
    "from IPython.display import clear_output\n",
    "\n",
    "#dynamic memory allocation\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_memory_growth(gpus[0], True)\n",
    "print(tf.__version__)\n",
    "env = gym.make('CartPole-v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cartpole Specific Model\n",
    "def create_cartmodel(input_shape, no_of_actions, lr):\n",
    "    model = tf.keras.Sequential([\n",
    "        Dense(24, input_shape = (input_shape, ), activation = \"relu\", kernel_initializer= \"he_uniform\"),\n",
    "        Dense(24, activation = \"relu\", kernel_initializer= \"he_uniform\"),\n",
    "        Dense(no_of_actions, activation = \"linear\", kernel_initializer= \"he_uniform\")\n",
    "    ])\n",
    "    model.compile(optimizer = Adam(lr = lr), loss = 'mse')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQL:\n",
    "\n",
    "    def __init__(self, env, model, epsilon = 1, decay_rate = 0.996, min_epsilon = 0.1, discount_factor = 0.95, name = 'Default'):\n",
    "        \"\"\"\n",
    "        Function which initializes all the values and parameters for the Deep Q Learning\n",
    "\n",
    "        Input:\n",
    "        env - Open AI Environment\n",
    "        model - Q learning model specific to the environment\n",
    "        epsilon - The rate of being greedy (Default = 1)\n",
    "        decay_rate - Epsilon decay rate (Default = 0.99)\n",
    "        min_epsilon - Minimum value of epsilon (Default = 0.1)\n",
    "        discount_factor - Gamma value for RL (Default = 1)\n",
    "        name - Model's Name (Default = 'Default)\n",
    "\n",
    "        Returns:\n",
    "        DQL Object\n",
    "        \"\"\"\n",
    "        self.name = name\n",
    "        self.env = env\n",
    "        self.model = model\n",
    "        self.epsilon = epsilon\n",
    "        self.decay_rate = decay_rate\n",
    "        self.discount_factor = discount_factor\n",
    "        self.min_epsilon = min_epsilon\n",
    "        self.er = []\n",
    "        self.rewards = []\n",
    "        self.mse = []\n",
    "        self.batch_size = 0\n",
    "\n",
    "    def predict(self, state):\n",
    "        '''\n",
    "        Helper function to reshape and predict\n",
    "        '''\n",
    "        return self.model.predict(state.reshape(1, -1))\n",
    "\n",
    "    def epsilon_policy(self, state):\n",
    "        '''\n",
    "        Function which finds the action probabilities based on epsilon policy given a specific state\n",
    "\n",
    "        Input:\n",
    "        state - The current state of the environment\n",
    "\n",
    "        Returns:\n",
    "        Probabilities of all the actions\n",
    "        ''' \n",
    "\n",
    "        if random.random() <= self.epsilon:\n",
    "            return random.randrange(self.env.action_space.n)\n",
    "        \n",
    "        else:\n",
    "            return np.argmax(self.predict(state)[0])\n",
    "\n",
    "    def plot(self, i, tot_reward, tot_loss):\n",
    "        '''\n",
    "        Function to plot mse and reward by epiode basis\n",
    "\n",
    "        Input:\n",
    "        i - Current episode number\n",
    "        tot_reward - Total reward for current episode\n",
    "        tot_loss - Total loss for current episode\n",
    "\n",
    "        '''\n",
    "        clear_output(wait = True)\n",
    "        fig, ax = plt.subplots(1, 2, figsize = (12, 5))\n",
    "        fig.suptitle(\"Episode : {} Reward: {} Loss: {} Epsilon: {}\".format(i, tot_reward, round(tot_loss, 3), round(self.epsilon, 3)))\n",
    "        ax[0].set_xlabel('Episodes')\n",
    "        ax[0].set_ylabel('Rewards')\n",
    "        ax[1].set_xlabel('Episodes')\n",
    "        ax[1].set_ylabel('MSE')\n",
    "        \n",
    "        ax[1].plot(self.mse)\n",
    "        ax[0].plot(self.rewards, label = 'Reward')\n",
    "        ax[0].legend()\n",
    "        ax[0].grid()\n",
    "        ax[1].grid()\n",
    "        plt.show()\n",
    "\n",
    "    def prepopulate_er(self):\n",
    "        '''\n",
    "        Function to prepopulate the experience replay by batch size\n",
    "        '''\n",
    "\n",
    "        state = self.env.reset()\n",
    "        for i in range(self.batch_size):\n",
    "\n",
    "            action = self.epsilon_policy(state)\n",
    "            next_state, reward, done, _ = self.env.step(action)\n",
    "            self.er.append((state, action, reward, next_state, done))\n",
    "\n",
    "            if done:\n",
    "                state = self.env.reset()\n",
    "            \n",
    "            else:\n",
    "                state = next_state\n",
    "\n",
    "    \n",
    "    def update_network(self):\n",
    "        '''\n",
    "        Function which updates the network to the target q values\n",
    "        '''\n",
    "\n",
    "        samples = random.sample(self.er, self.batch_size)\n",
    "        States = np.array([s[0] for s in samples])\n",
    "        Next_States = np.array([s[3] for s in samples])\n",
    "        \n",
    "        target = self.model.predict(States)\n",
    "        target_next = self.model.predict(Next_States)\n",
    "\n",
    "        for j,s in enumerate(samples):\n",
    "            state, action, reward, next_state, done = s\n",
    "            if done:\n",
    "                target[j][action] = reward\n",
    "            else:\n",
    "                target[j][action] = reward + (self.discount_factor * np.amax(target_next[j]))\n",
    "        \n",
    "        hist = self.model.fit(States, target, batch_size = self.batch_size, verbose = False)\n",
    "        return round(hist.history['loss'][0], 3)\n",
    "\n",
    "    def learn(self, noe, batch_size):\n",
    "        '''\n",
    "        Function which trains the agent\n",
    "\n",
    "        Input:\n",
    "        noe - No of episodes to train\n",
    "        batch_size - Experience replay batch train size\n",
    "        '''\n",
    "        \n",
    "        self.batch_size = batch_size\n",
    "        self.prepopulate_er()\n",
    "\n",
    "        for i in range(noe):\n",
    "            tot_reward = 0\n",
    "            tot_loss = 0\n",
    "            length = 0\n",
    "            self.epsilon = (max(self.min_epsilon, self.epsilon*self.decay_rate))\n",
    "            state = self.env.reset()\n",
    "            while True:\n",
    "                self.env.render()\n",
    "                action = self.epsilon_policy(state)\n",
    "                '''\n",
    "                if(action == 0): \n",
    "                    print(\"L\", end = ' ') \n",
    "                else: \n",
    "                    print(\"R\", end = ' ')\n",
    "                '''\n",
    "                next_state, reward, done, _ = self.env.step(action)\n",
    "\n",
    "                self.er.append((state, action, reward, next_state, done))\n",
    "                loss = self.update_network()\n",
    "                length += 1\n",
    "                tot_reward += reward\n",
    "                tot_loss += loss if loss else 0\n",
    "                if done:\n",
    "                    break\n",
    "                else:\n",
    "                    state = next_state\n",
    "\n",
    "            tot_loss/=length\n",
    "            self.rewards.append(tot_reward)\n",
    "            self.mse.append(tot_loss)\n",
    "            self.plot(i, tot_reward, tot_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = create_cartmodel(4, 2, 0.001)\n",
    "dql = DQL(env, model)\n",
    "dql.learn(1000, 64)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}