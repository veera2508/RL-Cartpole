{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python38364bitd28d5bfd303b43519c8e7d8f134c3bbd",
   "display_name": "Python 3.8.3 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "2.2.0\n"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import gym\n",
    "import random\n",
    "import itertools\n",
    "\n",
    "#dynamic memory allocation\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_memory_growth(gpus[0], True)\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "env.reset()\n",
    "for _ in range(200):\n",
    "    env.render()\n",
    "    env.step(env.action_space.sample())\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(4,)"
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "source": [
    "a = env.reset()\n",
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "def create_model(input_shape, no_of_actions):\n",
    "    model = tf.keras.Sequential([\n",
    "        Dense(64, input_shape = (input_shape, ), activation = tf.nn.relu),\n",
    "        Dense(32, activation = tf.nn.relu),\n",
    "        Dense(no_of_actions)\n",
    "    ])\n",
    "    model.compile(optimizer = Adam(lr = 0.001), loss = 'mean_squared_error')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epolicy(model, epsilon, nA, state):\n",
    "    best_action = np.argmax(model.predict(state.reshape(1,-1)))\n",
    "    probs = [epsilon/nA for i in range(nA)]\n",
    "    probs[best_action] += 1.0-epsilon\n",
    "    return probs \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deep_ql(no_of_episodes, epsilon, env, batch_size, decay_rate, discount_factor):\n",
    "    er = []\n",
    "    model = create_model(4, 2)\n",
    "    model.summary()\n",
    "    print(model.output_shape)\n",
    "\n",
    "    episode_length = []\n",
    "    episode_reward = []\n",
    "    state = env.reset()\n",
    "    for i in range(batch_size):\n",
    "        probs = epolicy(model, epsilon, 2, state)\n",
    "        action = np.random.choice(np.arange(len(probs)), p = probs)\n",
    "        print(action)\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        er.append((state, action, reward, next_state, done))\n",
    "\n",
    "        if done:\n",
    "            state = env.reset()\n",
    "        \n",
    "        else:\n",
    "            state = next_state\n",
    "\n",
    "    for i in range(no_of_episodes):\n",
    "        if i%50 == 0:\n",
    "            print(\"Episode no : \", i)\n",
    "        \n",
    "        state = env.reset()\n",
    "        for t in itertools.count():\n",
    "\n",
    "            epsilon = epsilon**decay_rate\n",
    "            if i%50 == 0:\n",
    "                env.render()\n",
    "            \n",
    "\n",
    "            probs = epolicy(model, epsilon, 2, state)\n",
    "            action = np.random.choice(np.arange(len(probs)), p = probs)\n",
    "            onext_state, reward, odone, _ = env.step(action)\n",
    "            er.append((state, action, reward, onext_state, odone))\n",
    "\n",
    "\n",
    "            samples = random.sample(er, batch_size)\n",
    "            States_tr = np.array([s[0] for s in samples])\n",
    "            Q_tr = np.zeros((batch_size, 2))\n",
    "            print(Q_tr.shape)\n",
    "            for j, s in enumerate(samples):\n",
    "                state, action, reward, next_state, done = s\n",
    "                Q_tr[j][action] = (reward + np.invert(done) * discount_factor * np.max(model.predict(next_state.reshape(1, -1))))\n",
    "            \n",
    "            model.fit(States_tr, Q_tr, epochs = 1, verbose = False)\n",
    "\n",
    "            if odone:\n",
    "                break\n",
    "            else:\n",
    "                state = onext_state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Model: \"sequential_39\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ndense_118 (Dense)            (None, 64)                320       \n_________________________________________________________________\ndense_119 (Dense)            (None, 32)                2080      \n_________________________________________________________________\ndense_120 (Dense)            (None, 2)                 66        \n=================================================================\nTotal params: 2,466\nTrainable params: 2,466\nNon-trainable params: 0\n_________________________________________________________________\n(None, 2)\n1\n0\n1\n0\n0\n0\n1\n0\n0\n0\n1\n1\n1\n0\n1\n1\n0\n1\n0\n1\n0\n1\n1\n1\n0\n0\n1\n0\n0\n1\n0\n0\nEpisode no :  0\n(32, 2)\n1/1 - 0s - loss: 0.5487\n(32, 2)\n1/1 - 0s - loss: 0.5454\n(32, 2)\n1/1 - 0s - loss: 0.5354\n(32, 2)\n1/1 - 0s - loss: 0.5416\n(32, 2)\n1/1 - 0s - loss: 0.5348\n(32, 2)\n1/1 - 0s - loss: 0.5264\n(32, 2)\n1/1 - 0s - loss: 0.5254\n(32, 2)\n1/1 - 0s - loss: 0.5279\n(32, 2)\n1/1 - 0s - loss: 0.5322\n(32, 2)\n1/1 - 0s - loss: 0.5352\n(32, 2)\n1/1 - 0s - loss: 0.5428\n(32, 2)\n1/1 - 0s - loss: 0.5639\n(32, 2)\n1/1 - 0s - loss: 0.5563\n(32, 2)\n1/1 - 0s - loss: 0.5571\n(32, 2)\n1/1 - 0s - loss: 0.5349\n(32, 2)\n1/1 - 0s - loss: 0.5807\n(32, 2)\n1/1 - 0s - loss: 0.5591\n(32, 2)\n1/1 - 0s - loss: 0.5682\n(32, 2)\n1/1 - 0s - loss: 0.5379\n(32, 2)\n1/1 - 0s - loss: 0.5198\n(32, 2)\n1/1 - 0s - loss: 0.5350\n(32, 2)\n1/1 - 0s - loss: 0.5591\n(32, 2)\n1/1 - 0s - loss: 0.5572\n(32, 2)\n1/1 - 0s - loss: 0.5793\n(32, 2)\n1/1 - 0s - loss: 0.5643\n(32, 2)\n1/1 - 0s - loss: 0.5817\n(32, 2)\n1/1 - 0s - loss: 0.5602\n(32, 2)\n1/1 - 0s - loss: 0.6226\n(32, 2)\n1/1 - 0s - loss: 0.6177\n(32, 2)\n1/1 - 0s - loss: 0.6128\n(32, 2)\n1/1 - 0s - loss: 0.6863\n(32, 2)\n1/1 - 0s - loss: 0.5974\n(32, 2)\n1/1 - 0s - loss: 0.6042\n(32, 2)\n1/1 - 0s - loss: 0.6165\n(32, 2)\n1/1 - 0s - loss: 0.6209\n(32, 2)\n1/1 - 0s - loss: 0.6512\n(32, 2)\n1/1 - 0s - loss: 0.6197\n(32, 2)\n1/1 - 0s - loss: 0.6023\n(32, 2)\n1/1 - 0s - loss: 0.6032\n(32, 2)\n1/1 - 0s - loss: 0.6170\n(32, 2)\n1/1 - 0s - loss: 0.6538\n(32, 2)\n1/1 - 0s - loss: 0.6202\n(32, 2)\n1/1 - 0s - loss: 0.6730\n(32, 2)\n1/1 - 0s - loss: 0.6480\n(32, 2)\n1/1 - 0s - loss: 0.6417\n(32, 2)\n1/1 - 0s - loss: 0.6840\n(32, 2)\n1/1 - 0s - loss: 0.6437\n(32, 2)\n1/1 - 0s - loss: 0.7559\n(32, 2)\n1/1 - 0s - loss: 0.6228\n(32, 2)\n1/1 - 0s - loss: 0.8087\n(32, 2)\n1/1 - 0s - loss: 0.8553\n(32, 2)\n1/1 - 0s - loss: 0.7664\n(32, 2)\n1/1 - 0s - loss: 0.7276\n(32, 2)\n1/1 - 0s - loss: 0.7493\n(32, 2)\n1/1 - 0s - loss: 0.6868\n(32, 2)\n1/1 - 0s - loss: 0.8833\n(32, 2)\n1/1 - 0s - loss: 0.8606\n(32, 2)\n1/1 - 0s - loss: 0.8506\n(32, 2)\n1/1 - 0s - loss: 0.8417\n(32, 2)\n1/1 - 0s - loss: 0.7848\n(32, 2)\n1/1 - 0s - loss: 0.8965\n(32, 2)\n1/1 - 0s - loss: 0.9785\n(32, 2)\n1/1 - 0s - loss: 0.7910\n(32, 2)\n1/1 - 0s - loss: 0.9189\n(32, 2)\n1/1 - 0s - loss: 0.9754\n(32, 2)\n1/1 - 0s - loss: 0.9762\n(32, 2)\n1/1 - 0s - loss: 0.9593\n(32, 2)\n1/1 - 0s - loss: 1.0862\n(32, 2)\n1/1 - 0s - loss: 0.9186\n(32, 2)\n1/1 - 0s - loss: 1.1584\n(32, 2)\n1/1 - 0s - loss: 1.0199\n(32, 2)\n1/1 - 0s - loss: 1.0117\n(32, 2)\n1/1 - 0s - loss: 1.2608\n(32, 2)\n1/1 - 0s - loss: 0.8692\n(32, 2)\n1/1 - 0s - loss: 1.0005\n(32, 2)\n1/1 - 0s - loss: 1.1529\n(32, 2)\n1/1 - 0s - loss: 1.2401\n(32, 2)\n1/1 - 0s - loss: 1.0634\n(32, 2)\n1/1 - 0s - loss: 1.0192\n(32, 2)\n1/1 - 0s - loss: 1.2739\n(32, 2)\n1/1 - 0s - loss: 1.3778\n(32, 2)\n1/1 - 0s - loss: 1.4506\n(32, 2)\n1/1 - 0s - loss: 1.1918\n(32, 2)\n1/1 - 0s - loss: 1.3206\n(32, 2)\n1/1 - 0s - loss: 1.3464\n(32, 2)\n1/1 - 0s - loss: 1.5099\n(32, 2)\n1/1 - 0s - loss: 1.1022\n(32, 2)\n1/1 - 0s - loss: 1.1514\n(32, 2)\n1/1 - 0s - loss: 1.4652\n(32, 2)\n1/1 - 0s - loss: 1.3946\n(32, 2)\n1/1 - 0s - loss: 1.7584\n(32, 2)\n1/1 - 0s - loss: 1.9810\n(32, 2)\n1/1 - 0s - loss: 1.7502\n(32, 2)\n1/1 - 0s - loss: 1.4568\n(32, 2)\n1/1 - 0s - loss: 1.9741\n(32, 2)\n1/1 - 0s - loss: 1.6637\n(32, 2)\n1/1 - 0s - loss: 1.7332\n(32, 2)\n1/1 - 0s - loss: 2.2732\n(32, 2)\n1/1 - 0s - loss: 1.2682\n(32, 2)\n1/1 - 0s - loss: 1.3573\n(32, 2)\n1/1 - 0s - loss: 1.8983\n(32, 2)\n1/1 - 0s - loss: 1.9277\n(32, 2)\n1/1 - 0s - loss: 1.3693\n(32, 2)\n1/1 - 0s - loss: 1.3468\n(32, 2)\n1/1 - 0s - loss: 1.3883\n(32, 2)\n1/1 - 0s - loss: 1.8302\n(32, 2)\n1/1 - 0s - loss: 1.6340\n(32, 2)\n1/1 - 0s - loss: 1.7867\n(32, 2)\n1/1 - 0s - loss: 1.7449\n(32, 2)\n1/1 - 0s - loss: 1.7963\n(32, 2)\n1/1 - 0s - loss: 1.9274\n(32, 2)\n1/1 - 0s - loss: 1.3355\n(32, 2)\n1/1 - 0s - loss: 1.8295\n(32, 2)\n1/1 - 0s - loss: 1.6072\n(32, 2)\n1/1 - 0s - loss: 1.7133\n(32, 2)\n1/1 - 0s - loss: 1.3991\n(32, 2)\n1/1 - 0s - loss: 1.5266\n(32, 2)\n1/1 - 0s - loss: 1.6191\n(32, 2)\n1/1 - 0s - loss: 1.6203\n(32, 2)\n1/1 - 0s - loss: 1.3936\n(32, 2)\n1/1 - 0s - loss: 1.6938\n(32, 2)\n1/1 - 0s - loss: 1.6858\n(32, 2)\n1/1 - 0s - loss: 1.4899\n(32, 2)\n1/1 - 0s - loss: 1.6830\n(32, 2)\n1/1 - 0s - loss: 1.4401\n(32, 2)\n1/1 - 0s - loss: 1.7004\n(32, 2)\n1/1 - 0s - loss: 1.5138\n(32, 2)\n1/1 - 0s - loss: 1.4009\n(32, 2)\n1/1 - 0s - loss: 1.4681\n(32, 2)\n1/1 - 0s - loss: 1.4712\n(32, 2)\n1/1 - 0s - loss: 1.5256\n(32, 2)\n1/1 - 0s - loss: 1.4704\n(32, 2)\n1/1 - 0s - loss: 1.5730\n(32, 2)\n1/1 - 0s - loss: 1.4489\n(32, 2)\n1/1 - 0s - loss: 1.3743\n(32, 2)\n1/1 - 0s - loss: 1.3892\n(32, 2)\n1/1 - 0s - loss: 1.3764\n(32, 2)\n1/1 - 0s - loss: 1.4202\n(32, 2)\n1/1 - 0s - loss: 1.3171\n(32, 2)\n1/1 - 0s - loss: 1.1940\n(32, 2)\n1/1 - 0s - loss: 1.4000\n(32, 2)\n1/1 - 0s - loss: 1.4886\n(32, 2)\n1/1 - 0s - loss: 1.4495\n(32, 2)\n1/1 - 0s - loss: 1.4376\n(32, 2)\n1/1 - 0s - loss: 1.2720\n(32, 2)\n1/1 - 0s - loss: 1.0601\n(32, 2)\n1/1 - 0s - loss: 1.3292\n(32, 2)\n1/1 - 0s - loss: 1.3813\n(32, 2)\n1/1 - 0s - loss: 1.2138\n(32, 2)\n1/1 - 0s - loss: 1.1807\n(32, 2)\n1/1 - 0s - loss: 1.2783\n(32, 2)\n1/1 - 0s - loss: 1.0422\n(32, 2)\n1/1 - 0s - loss: 1.1818\n(32, 2)\n1/1 - 0s - loss: 1.2571\n(32, 2)\n1/1 - 0s - loss: 1.0938\n(32, 2)\n1/1 - 0s - loss: 1.1298\n(32, 2)\n1/1 - 0s - loss: 1.1257\n(32, 2)\n1/1 - 0s - loss: 1.0165\n(32, 2)\n1/1 - 0s - loss: 1.1834\n(32, 2)\n1/1 - 0s - loss: 1.2199\n(32, 2)\n1/1 - 0s - loss: 1.1912\n(32, 2)\n1/1 - 0s - loss: 1.0516\n(32, 2)\n1/1 - 0s - loss: 1.0902\n(32, 2)\n1/1 - 0s - loss: 1.0936\n(32, 2)\n1/1 - 0s - loss: 1.2107\n(32, 2)\n1/1 - 0s - loss: 1.2130\n(32, 2)\n1/1 - 0s - loss: 1.0941\n(32, 2)\n1/1 - 0s - loss: 1.1450\n(32, 2)\n1/1 - 0s - loss: 1.3429\n(32, 2)\n1/1 - 0s - loss: 1.2314\n(32, 2)\n1/1 - 0s - loss: 1.0184\n(32, 2)\n1/1 - 0s - loss: 1.0352\n(32, 2)\n1/1 - 0s - loss: 1.2593\n(32, 2)\n1/1 - 0s - loss: 1.2488\n(32, 2)\n1/1 - 0s - loss: 1.1205\n(32, 2)\n1/1 - 0s - loss: 0.9818\n(32, 2)\n1/1 - 0s - loss: 1.2436\n(32, 2)\n1/1 - 0s - loss: 1.1464\n(32, 2)\n1/1 - 0s - loss: 1.2633\n(32, 2)\n1/1 - 0s - loss: 1.0156\n(32, 2)\n1/1 - 0s - loss: 1.0634\n(32, 2)\n1/1 - 0s - loss: 1.1190\n(32, 2)\n1/1 - 0s - loss: 1.2051\n(32, 2)\n1/1 - 0s - loss: 1.2922\n(32, 2)\n1/1 - 0s - loss: 1.0261\n(32, 2)\n1/1 - 0s - loss: 1.1128\n(32, 2)\n1/1 - 0s - loss: 1.2891\n(32, 2)\n1/1 - 0s - loss: 1.1377\n(32, 2)\n1/1 - 0s - loss: 1.1880\n(32, 2)\n1/1 - 0s - loss: 1.0921\n(32, 2)\n1/1 - 0s - loss: 1.2085\n(32, 2)\n1/1 - 0s - loss: 1.1052\n(32, 2)\n1/1 - 0s - loss: 1.2666\n(32, 2)\n1/1 - 0s - loss: 1.1220\n(32, 2)\n1/1 - 0s - loss: 1.2035\n(32, 2)\n1/1 - 0s - loss: 1.1878\n(32, 2)\n1/1 - 0s - loss: 1.2432\n(32, 2)\n1/1 - 0s - loss: 1.1523\n(32, 2)\n1/1 - 0s - loss: 1.0170\n(32, 2)\n1/1 - 0s - loss: 1.2392\n(32, 2)\n1/1 - 0s - loss: 1.1082\n(32, 2)\n1/1 - 0s - loss: 1.0499\n(32, 2)\n1/1 - 0s - loss: 1.2514\n(32, 2)\n1/1 - 0s - loss: 1.1475\n(32, 2)\n1/1 - 0s - loss: 1.2542\n(32, 2)\n1/1 - 0s - loss: 1.0783\n(32, 2)\n1/1 - 0s - loss: 1.1222\n(32, 2)\n1/1 - 0s - loss: 1.1733\n(32, 2)\n1/1 - 0s - loss: 1.2514\n(32, 2)\n1/1 - 0s - loss: 1.1317\n(32, 2)\n1/1 - 0s - loss: 1.2484\n(32, 2)\n1/1 - 0s - loss: 1.2663\n(32, 2)\n1/1 - 0s - loss: 1.0906\n(32, 2)\n1/1 - 0s - loss: 1.0094\n(32, 2)\n1/1 - 0s - loss: 1.0522\n(32, 2)\n1/1 - 0s - loss: 1.1587\n(32, 2)\n1/1 - 0s - loss: 1.1492\n(32, 2)\n1/1 - 0s - loss: 1.1188\n(32, 2)\n1/1 - 0s - loss: 1.2167\n(32, 2)\n1/1 - 0s - loss: 1.1328\n(32, 2)\n1/1 - 0s - loss: 1.0721\n(32, 2)\n1/1 - 0s - loss: 1.1019\n(32, 2)\n1/1 - 0s - loss: 1.1111\n(32, 2)\n1/1 - 0s - loss: 1.1792\n(32, 2)\n1/1 - 0s - loss: 1.1169\n(32, 2)\n1/1 - 0s - loss: 1.1352\n(32, 2)\n1/1 - 0s - loss: 1.0280\n(32, 2)\n1/1 - 0s - loss: 1.1433\n(32, 2)\n1/1 - 0s - loss: 1.0693\n"
    }
   ],
   "source": [
    "deep_ql(10, 0.8, env, 32, 0.99, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}